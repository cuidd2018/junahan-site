# -*- mode: org; coding: utf-8; -*-
#+TITLE:                    Deep Learning 读书笔记
#+AUTHOR:                   Junahan
#+EMAIL:                    junahan@outlook.com
#+DATE:                     2019-01-01
#+hugo_base_dir:            ../
#+hugo_auto_set_lastmod:    t
#+hugo_tags:                "Deep Learning" 深度学习
#+hugo_categories:          "Deep Learning" 深度学习
#+hugo_draft:               true
#+KEYWORDS:                 "Deep Learning" 深度学习
#+LANGUAGE:                 CN
#+OPTIONS:                  H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:                  TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT:               view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+LICENSE:                  CC BY 4.0

* 摘要
本文为《深度学习》(Deep Learning) 一书的读书笔记。该书由全球知名的三位人工智能方面专家 Ian Goodfellow, Yoshua Bengio 和 Arron Courville 联合撰写。成书于 2016 年，恰逢人工智能第三次浪潮[fn:1] (始于 2006 年)。

* 机器学习基础
** TODO 学习算法

** TODO 模型容量、过拟合和欠拟合

** 超参数和验证集
- 大多数学习算法都有超参数，可以设置来控制算法行为。例如，学习率 (Learning Rate) 
- 超参数通常不适合在训练集上学习，例如用于控制模型容量的超参数 (如权重衰减系数) ，如果在训练集上学习，这些超参数总是倾向于最大可能的模型容量，从而导致过拟合
- 测试集不能用于挑选超参数，这和测试集不能用于训练一个道理
- 为了挑选超参数，我们需要一个训练算法观测不到的验证集 (validation set) ，通常，我们从训练集里面分出 20% 作为验证集
- 如果只有一个超参数需要调，就调整学习率

** 估计、偏差和方差
- 偏差和方差的关系与机器学习容量、过拟合和欠拟合的概念密切相关。使用 MSE 度量泛化误差时，增加容量会增加方差、降低偏差。这也是容量函数中泛化误差是 U 型曲线的原因。

** TODO 最大似然估计
- 

** 随机梯度下降
请参阅[[*%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95][随机梯度下降算法]]一节。

** 构建机器学习算法
几乎所有深度学习算法都可以被描述为一个简单的配方：
- 特定的数据集
- 代价函数
- 优化过程
- 模型

如线性回归算法有如下部分组成：
- 由 X 和 y 构成的数据集
- 代价函数：$J(w,b) = E_{x,y \sim \hat{p}_{data}}log p_{model}(y|x)$
- 模型：$p_{model}(y|x) = \mathcal N (y;x^T w + b, 1)$
- 优化算法：梯度下降

组合模型、代价函数和优化算法来构建学习算法的配方同样适用于监督学习和无监督学习。

** 促使深度学习发展的挑战
*** 维数灾难 (curse of dimensionality)
当数据的维数很高时，很多机器学习问题变得相当困难。这种现象成为维数灾难。
- 一组变量不同的可能配置数量会随着变量数目的增长而指数级增长
- 从而产生统计上的挑战：样本数目远远小于可能的配置数量

*** 局部不变形和平滑正则化
为了更好的泛化，机器学习算法需要有先验信念引导应该学习什么类型的函数。最广泛的隐式先验是平滑先验 (smoothness prior) 或局部不变先验 (local constancy prior)，这个先验表明我们学习的函数不应在小区域内发生很大的变化。表现为假设 $f^*(x) \approx f^*(x + \epsilon)$ 。

*** 流形学习和流形假设
流形学习 (manifold learning) 算法假设高维空间 $\mathbb R^{n}$ 中大部分区域都是无效的输入，有意义的输入只分布在包含少数数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。

支持流形假设来自于两个观察：
- 现实生活中的图像、文本、声音的概率分布都是高度集中的。均匀噪声从来不会与这类领域的结构化输入类似。如，自然语言中序列的分布只占了字母序列的总空间里非常小的一部分。
- 这些高度集中的样本相互连接，每个样本都被其他高度相似的样本包围，而这些高度相似的样本可以通过变换来遍历该流形得到。如图像中，有很多可能的变换允许我们描绘出图片空间的流形：如移动和旋转图片，改变图片颜色、亮度等。

*** 小结
深度学习的核心思想是假设数据由因素 () 或者特征组合 () 产生，这些因素和特征可能来自于一个层次结构的多个层级。许多其他类似的通用假设进一步提高了深度学习算法。这些很温和的假设允许样本数目和可区分区间数目之间的指数增益。深度的分布式表示带来的指数增益可以有效的解决维数灾难带来的挑战。

* 深度前馈网络


* 梯度下降算法
这里将梯度下降算法单独摘要汇总。

** 随机梯度下降算法
几乎所有深度学习算法都用到了一个非常重要的算法：随机梯度下降 (stochastic gradient descent, SGD)。

机器学习算法中的代价函数通常可以分解为每个样本的代价函数的总和。例如，训练数据的负对数似然可以写成：

\begin{equation}
J(\theta) = E_{x,y \sim \hat{p}_{data}}L(x,y,\theta) = \frac{1}{m} \sum^{m}_{i=1} L(x^{(i)}, y^{(i)}, \theta)
\end{equation}

其中:
- 每个样本的损失 $L(x,y, \theta) = - log p(y | x; \theta)$ 
- $\hat{p}_{data}$ 代表在训练集中的经验分布以区分真实分布 $p_{data}$ 

对于以上相加的代价函数，梯度下降需要计算：

\begin{equation}
\nabla_{\theta}J(\theta) = \frac{1}{m} \sum^{m}_{i=1} \nabla_{\theta}L(x^{(i)},y^{(i)},\theta)
\end{equation}

这个运算的代价是 $O(m)$ 。随着训练集规模增长为数十亿样本，计算一步梯度也会消耗相当长时间。

随机梯度下降的核心是，梯度是期望。期望可以使用小规模的样本近似估计。通过从训练集中均匀抽取一小批 (minibatch) 样本 $\mathbb{B} = {x^{(1)}, \cdots , x^{(m^{\prime})}}$ 。其中，小批量 $m^{\prime}$ 通常是一个较小的数，从一到几百。这样，每次更新计算只要计算几百个样本。梯度的估计可以表示为：

\begin{equation}
g = \frac{1}{m^\prime} \nabla_{\theta} \sum^{m^\prime}_{i=1}L(x^{(i)},y^{(i)},\theta)
\end{equation}

使用来自小批量 $\mathbb{B}$ 的样本，随机梯度下降可以使用如下估算：

\begin{equation}
\theta \xleftarrow{} \theta - \epsilon g
\end{equation}

其中 $\epsilon$ 是学习率。

随机梯度下降算法的几个要点：
- 随机梯度下降的核心是，梯度是期望。期望可以使用小规模的样本近似估计。
- 优化算法不一定能够保证在合理的时间内达到一个局部最小值，但它通常能够及时找到代价函数一个很小的值，且改值是有用的。
- 随机梯度下降在深度学习之外也有很多重要的应用。它是在大规模数据集上训练大型线性模型的主要方法。一般而言，训练算法收敛所需要的更新次数随着训练集的规模增大而增大，然而，随着训练集规模趋向于无穷，该模型最终会在随机梯度下降抽样完训练集上所有样本前收敛到可能的最优测试误差。继续增加训练集规模不会延长模型的训练时间。

* Footnotes

[fn:1] 人工智能的第三次浪潮，大约始于 2006 年，以 Hiton et al., 2006; Bengio et al., 2007a; Ranzato et al., 2007a 的论文为代表，以深度学习的名义回归。



